{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restaurant Entrepreneur:  Predicting best type and location with Yelp Review Data\n",
    "\n",
    "Bill (an investor) went on a work trip recently to Las Vegas and Phoenix for a few days. During his stay, he really liked the restaurant options that were available. Being that he already had interests in opening up a restaurant for some time now, he wanted to know what the type of restaurant and where to open would be most successful.\n",
    "\n",
    "As a first look, our group decided to use Yelp API to collect data on the mix of restaurants across cities and states within the United States. In this case, we decided to use the review data that Yelp is known for. Since Yelp provides a wide variety of restaurant categories, it is possible to get more information in regards to reviews and ratings for all different types in each city. After our initial attempts to use Yelp API, we found issues with importing the Yelp API data. When we created a dataframe from the API, it would only pull a small portion of the data on each run, not a sufficient amount of data to do a full analysis on. Fortunately, we were able to find some recent Yelp Review data on Kaggle.com that had a huge dataset to work with. \n",
    "\n",
    "Before deciding on which Machine Learning Model we were going to choose to do our analysis on, we wanted to test a few to see what would be the best. We began setting up our code by importing various libraries, which included Random Forest Classifier and DeepLearning Machine Learning Models. Random Forest Classifier is a good model if you want high performance with less need for interpretation. Deep Learning Model is know for it's supremacy in terms of accuracy when trained with huge amounts of data and to get more neural network predictions. We've also imported train_test_split which will help us split our data for training and testing.\n",
    "\n",
    "For the preprocessing, we imported StandardScaler and OneHotEncoder. The StandardScaler is needed to transform the data so that it has a mean of 0 and a standard deviation of 1. The OneHotEncoder is needed as it creates a binary column for each category type of restaurant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest and DeepLearning\n",
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cleaned data is loaded into postgres database. It is also formatted in the format required for ML during transformation process.\n",
    "# We are trying random forest classifier as the data will be divided into smaller sets and prediction could be near to accuracy\n",
    "# We are also adding deep learning to get more neural network predition\n",
    "# Based on the line identified, the output variable will be predicted for the input vairable\n",
    "# Once the complete dataset is loaded and the accuracy is identified, we will pick the best approch. This should be sometime in next session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned data is loaded into a postgres database. It is also formatted to fit the required format for Machine Learning during the transformation process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull data from busiensses table from postgres\n",
    "engine = create_engine('postgresql+psycopg2://postgres:postgres@localhost/Yelp_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import Session\n",
    "session = Session(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviewsDF = pd.read_sql('select stars,city,state,postal_code,category from reviews r, businesses b where b.business_id = r.business_id and length(postal_code)>0',engine)\n",
    "reviewsDF = pd.read_sql ('select b.stars , b.city, b.postal_code, r.ethnic_type from business_reviews r, business_info b where b.business_id = r.business_id and length(b.postal_code)>0 group by b.stars,b.city,b.postal_code, r.ethnic_type,useful order by b.postal_code',engine)\n",
    "XInput =reviewsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = reviewsDF.stars\n",
    "yDF=round(pd.DataFrame(y))\n",
    "X = reviewsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89109    1345\n",
       "89102     825\n",
       "89119     708\n",
       "89103     644\n",
       "89146     610\n",
       "         ... \n",
       "85388       3\n",
       "89018       3\n",
       "93013       2\n",
       "85303       2\n",
       "89161       2\n",
       "Name: postal_code, Length: 108, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postalCountsX=X.postal_code.value_counts()\n",
    "postalCountsX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "American         4194\n",
       "Mexican          3436\n",
       "Italian          2194\n",
       "Asian_Fusion     1620\n",
       "Chinese          1616\n",
       "Japanese         1427\n",
       "Thai              784\n",
       "Mediterranean     666\n",
       "Greek             590\n",
       "Vietnamese        522\n",
       "Hawaiian          517\n",
       "French            508\n",
       "Indian            436\n",
       "Korean            327\n",
       "Spanish           207\n",
       "British           150\n",
       "African            79\n",
       "Name: ethnic_type, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoryCountsX=X.ethnic_type.value_counts()\n",
    "categoryCountsX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_type=list(categoryCountsX[categoryCountsX<100].index)\n",
    "replace_postalcode=list(postalCountsX[postalCountsX<100].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>city</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>ethnic_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.5</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Combined</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.5</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Combined</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.5</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Combined</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Combined</td>\n",
       "      <td>American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Combined</td>\n",
       "      <td>American</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars     city postal_code ethnic_type\n",
       "0    4.5  Phoenix    Combined     Italian\n",
       "1    4.5  Phoenix    Combined     Italian\n",
       "2    4.5  Phoenix    Combined     Italian\n",
       "3    5.0  Phoenix    Combined    American\n",
       "4    5.0  Phoenix    Combined    American"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for application in replace_type:\n",
    "    X.ethnic_type =  X.ethnic_type.replace(application,\"Others\")\n",
    "for application in replace_postalcode:\n",
    "    X.postal_code =  X.postal_code.replace(application,\"Combined\")    \n",
    "X.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city            2\n",
       "postal_code    64\n",
       "ethnic_type    17\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate our categorical variable list\n",
    "reviewCatX = X.dtypes[X.dtypes == \"object\"].index.tolist()\n",
    "X[reviewCatX].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate our categorical variable list\n",
    "reviewCaty = yDF.dtypes[yDF.dtypes == \"int64\"].index.tolist()\n",
    "yDF[reviewCaty].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictInputDF = pd.DataFrame(X.groupby(['stars','postal_code','city','ethnic_type']).sum()).reset_index()\n",
    "predictInputDF['stars'] = round(predictInputDF['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns=['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>city</th>\n",
       "      <th>ethnic_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85004</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85022</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85032</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85033</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85040</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Combined</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Combined</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Combined</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Combined</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Combined</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1725 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      stars postal_code       city ethnic_type\n",
       "0       1.0       85004    Phoenix     Chinese\n",
       "1       1.0       85022    Phoenix     Italian\n",
       "2       1.0       85032    Phoenix      Indian\n",
       "3       1.0       85033    Phoenix     Italian\n",
       "4       1.0       85040    Phoenix     Mexican\n",
       "...     ...         ...        ...         ...\n",
       "1720    5.0    Combined  Las Vegas     Mexican\n",
       "1721    5.0    Combined    Phoenix    American\n",
       "1722    5.0    Combined    Phoenix    Japanese\n",
       "1723    5.0    Combined    Phoenix     Mexican\n",
       "1724    5.0    Combined    Phoenix     Spanish\n",
       "\n",
       "[1725 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictInputDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postal_code</th>\n",
       "      <th>city</th>\n",
       "      <th>ethnic_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85004</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85022</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85032</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85033</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85040</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>Combined</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Hawaiian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>85029</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>89103</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>89121</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>Combined</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>641 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     postal_code       city ethnic_type\n",
       "0          85004    Phoenix     Chinese\n",
       "1          85022    Phoenix     Italian\n",
       "2          85032    Phoenix      Indian\n",
       "3          85033    Phoenix     Italian\n",
       "4          85040    Phoenix     Mexican\n",
       "...          ...        ...         ...\n",
       "1645    Combined    Phoenix    Hawaiian\n",
       "1665       85029    Phoenix        Thai\n",
       "1682       89103  Las Vegas      Others\n",
       "1700       89121  Las Vegas     Spanish\n",
       "1724    Combined    Phoenix     Spanish\n",
       "\n",
       "[641 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictInputDF=predictInputDF.drop(columns=['stars'])\n",
    "predictInputDF=predictInputDF.drop_duplicates()\n",
    "XInput=predictInputDF\n",
    "XInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training/test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, yDF, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encodeDFX_train = pd.DataFrame(enc.fit_transform(X_train[reviewCatX]))\n",
    "# Add the encoded variable names to the DataFrame\n",
    "encodeDFX_train.columns = enc.get_feature_names(reviewCatX)\n",
    "\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encodeDFX_test = pd.DataFrame(enc.fit_transform(X_test[reviewCatX]))\n",
    "# Add the encoded variable names to the DataFrame\n",
    "encodeDFX_test.columns = enc.get_feature_names(reviewCatX)\n",
    "#INput data fit\n",
    "encodeInputX = pd.DataFrame(enc.fit_transform(XInput[reviewCatX]))\n",
    "encodeInputX.columns = enc.get_feature_names(reviewCatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capply one hot on target labels\n",
    "enc.fit(y_train)\n",
    "#encodeDFy_train = pd.DataFrame(enc.fit_transform(y_train[reviewCaty]))\n",
    "#encodeDFy_test = pd.DataFrame(enc.fit_transform(y_test[reviewCaty]))\n",
    "encoded_y_train = enc.transform(y_train)                           \n",
    "encoded_y_test = enc.transform(y_test)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge one-hot encoded features and drop the originals\n",
    "# TestDF = reviewsDF.merge(encodeDF,left_index=True, right_index=True)\n",
    "# reviewsDF = reviewsDF.drop(reviewCat,1)\n",
    "# reviewsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(encodeDFX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the dataencodeDFX_train\n",
    "X_train_scaled = X_scaler.transform(encodeDFX_train)\n",
    "X_test_scaled = X_scaler.transform(encodeDFX_test)\n",
    "XInput_scaled = X_scaler.transform(encodeInputX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(641, 83)\n"
     ]
    }
   ],
   "source": [
    "print(XInput_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the first layer where the input dimensions are the 561 columns of the training data\n",
    "model.add(Dense(100, activation='relu', input_dim=X_train_scaled.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0    10794\n",
       "3.0     1783\n",
       "2.0     1582\n",
       "5.0      282\n",
       "1.0       13\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The output layer has 5 columns that are one-hot encoded\n",
    "y_train.stars.value_counts()\n",
    "#number_outputs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add output layer using 5 output nodes\n",
    "model.add(Dense(5, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model using categorical_crossentropy for the loss function, the adam optimizer,\n",
    "# and add accuracy to the training metrics\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               8400      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 8,905\n",
      "Trainable params: 8,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14454 samples\n",
      "Epoch 1/30\n",
      "14454/14454 - 1s - loss: 0.7725 - accuracy: 0.7379\n",
      "Epoch 2/30\n",
      "14454/14454 - 1s - loss: 0.6958 - accuracy: 0.7469\n",
      "Epoch 3/30\n",
      "14454/14454 - 1s - loss: 0.6728 - accuracy: 0.7498\n",
      "Epoch 4/30\n",
      "14454/14454 - 1s - loss: 0.6602 - accuracy: 0.7511\n",
      "Epoch 5/30\n",
      "14454/14454 - 1s - loss: 0.6497 - accuracy: 0.7541\n",
      "Epoch 6/30\n",
      "14454/14454 - 1s - loss: 0.6407 - accuracy: 0.7562\n",
      "Epoch 7/30\n",
      "14454/14454 - 1s - loss: 0.6331 - accuracy: 0.7572\n",
      "Epoch 8/30\n",
      "14454/14454 - 1s - loss: 0.6266 - accuracy: 0.7577\n",
      "Epoch 9/30\n",
      "14454/14454 - 1s - loss: 0.6216 - accuracy: 0.7578\n",
      "Epoch 10/30\n",
      "14454/14454 - 1s - loss: 0.6200 - accuracy: 0.7567\n",
      "Epoch 11/30\n",
      "14454/14454 - 1s - loss: 0.6147 - accuracy: 0.7592\n",
      "Epoch 12/30\n",
      "14454/14454 - 1s - loss: 0.6128 - accuracy: 0.7603\n",
      "Epoch 13/30\n",
      "14454/14454 - 1s - loss: 0.6097 - accuracy: 0.7610\n",
      "Epoch 14/30\n",
      "14454/14454 - 1s - loss: 0.6081 - accuracy: 0.7596\n",
      "Epoch 15/30\n",
      "14454/14454 - 1s - loss: 0.6056 - accuracy: 0.7615\n",
      "Epoch 16/30\n",
      "14454/14454 - 1s - loss: 0.6050 - accuracy: 0.7604\n",
      "Epoch 17/30\n",
      "14454/14454 - 1s - loss: 0.6058 - accuracy: 0.7588\n",
      "Epoch 18/30\n",
      "14454/14454 - 1s - loss: 0.6033 - accuracy: 0.7594\n",
      "Epoch 19/30\n",
      "14454/14454 - 1s - loss: 0.6033 - accuracy: 0.7612\n",
      "Epoch 20/30\n",
      "14454/14454 - 1s - loss: 0.6009 - accuracy: 0.7596\n",
      "Epoch 21/30\n",
      "14454/14454 - 1s - loss: 0.5993 - accuracy: 0.7606\n",
      "Epoch 22/30\n",
      "14454/14454 - 1s - loss: 0.6009 - accuracy: 0.7591\n",
      "Epoch 23/30\n",
      "14454/14454 - 1s - loss: 0.6001 - accuracy: 0.7584\n",
      "Epoch 24/30\n",
      "14454/14454 - 1s - loss: 0.5963 - accuracy: 0.7626\n",
      "Epoch 25/30\n",
      "14454/14454 - 1s - loss: 0.5984 - accuracy: 0.7617\n",
      "Epoch 26/30\n",
      "14454/14454 - 1s - loss: 0.5969 - accuracy: 0.7600\n",
      "Epoch 27/30\n",
      "14454/14454 - 1s - loss: 0.5971 - accuracy: 0.7625\n",
      "Epoch 28/30\n",
      "14454/14454 - 1s - loss: 0.5974 - accuracy: 0.7604\n",
      "Epoch 29/30\n",
      "14454/14454 - 1s - loss: 0.5949 - accuracy: 0.7625\n",
      "Epoch 30/30\n",
      "14454/14454 - 1s - loss: 0.5955 - accuracy: 0.7604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f012b5db88>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    encoded_y_train,\n",
    "    epochs=30,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4819/4819 [==============================] - 0s 49us/sample - loss: 0.6244 - accuracy: 0.7485\n",
      "Loss: 0.6243524110262808, Accuracy: 0.7484955191612244\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy =model.evaluate(X_test_scaled,encoded_y_test)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_output = model.predict(XInput_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(641, 5)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predictInputDF['prediction'] = pd.DataFrame(np.argmax(y_pred_output, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictInputDF[\"prediction\"] = predictInputDF[\"prediction\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data into postgres\n",
    "predictInputDF.to_sql(name='review_prediction', con=engine, if_exists='replace' ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(predictInputDF['stars'],predictInputDF['prediction']))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
